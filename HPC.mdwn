# High Performance Computing

> Mac Version  //  Update: 30/03/20 Adapted from [Beginner's guide to HPC at Imperial](https://imperialcollegelondon.app.box.com/s/fsoiljuf43f7ko4tfvqifbyrfj8yslbi) 


---


This will provide an overview of using the High Performance Computing (HPC) service offered by Imperial. The Research Computing Service at Imperial offers several services including acces to HPC and reserach data storage (RDS). 


---





 * [Access](#access)
    * [Ethernet](#ethernet)
    * [VPN](#vpn) 
 * [Why use HPC?](#why-use-hpc?)
 * [Using the Cluster](#using-the-cluster)
   * [Operating in Terminal](#operating-in-terminal)
      * [Loading Modules](#loading-modules)
      * [My Recommendations](#my-recommendations)
   * [Running Software](#running-software)
      * [Job Duration Requirement](#job-duration-requirement)
      * [Memory Requirement](#memory-requirement)
      * [Number of Parallel Processes](#number-of-parallel-processes)
      * [Job Sizing](#job-sizing)
   * [Working Directory for a Job](#working-directory-for-a-job) 
   * [Using the HPC Resource via RStudio](#using-the-hpc-resource-via-rstudio)
   * [Troubleshooting your Jobs](#troubleshooting-your-jobs)
   * [Parallel Computing](#parallel-computing)
 * [Globus](#globus)
 * [HPC Courses](#hpc-courses)
 * [Additional Information](#additional-information)
  


---


## Access

You will need to have access to the [Research Computing Service](https://api.rcs.imperial.ac.uk/groups/manage/hpc-access/). Simone already has an account `hpc-sdigiova`. If you are a post-doc, PhD or research postgrad student, you will need to ask Ilaria or Simone to register you.

> Do this first: [Getting started](https://www.imperial.ac.uk/admin-services/ict/self-service/research-support/rcs/support/getting-started/).

***You will also need access to the Imperial network***. 



### Ethernet 

If you are on Campus, I recommend connecting directly to the Imperial network via an ethernet cable. For Mac, since the new laptops only have a USB-C connection, you will need a USB-C to ethernet adaptor - you can buy these at any Apple store or [online](https://www.amazon.co.uk/Belkin-F2CU040btBLK-Ethernet-Certified-Compatible/dp/B014FBQ738). 

Next, you will need to register the adaptor with ICT. Use ICT Ask to submit a request or visit them during their drop-in hours (13:00-14:00). 

Once you have access, you can connect to the [research data store](https://www.imperial.ac.uk/admin-services/ict/self-service/research-support/rcs/rds/) by going to: 


   `Go > Connect to Server > smb://rds.imperial.ac.uk/RDS/user/yourusername > connect`


If you still need help, check out [this video](https://vimeo.com/302461588).


   > Note: ICT is located in the basement of the Common Wealth Building. If you use the elecators, turn right as you come out of them, then turn left. Go all the way to the end of the hallway, go through the doors, turn right and go through the doors again. Their office is the first door on the right. There should be a sign saying ICT office. 



### VPN

If you are working from home, you will need access to the Imperial Network via VPN. I use both [Tunnelblick](https://tunnelblick.net) and [Viscosity](https://www.sparklabs.com/viscosity/). But note that with Viscosity, you only have a 30 day trial - maybe we can purchase a license for the lab later on. I use Tunnelblick to access files via [FileZilla](https://filezilla-project.org). This is a great way to view and organise your files. Once you have FileZilla downloaded and open, where it says Host enter: `sftp://login.hpc.ic.ac.uk`, under Username enter `yourusername` and enter your password under Password. Leave Port blank. Now click Quickconnect. 



![FileZilla](images/FileZilla_Image.png)




**You can also use the command line:**

To copy files form the current directory on your machine to your home directory on RDS:

      ```zsh
      scp file.txt username@login.hpc.ic.ac.uk:~
      ```

To copy files from your home directory on RDS to the current directoy on your machine:

      ```zsh
      scp username@login.hpc.ic.ac.uk:~/file2.txt
      ```

e.g. `scp/Users/katerina/Desktop/file3.txt  username@login.hpc.ic.ac.uk:/rds/general/user/kmichali/home/projectx`


   > Note: for all these options, you need to have VPN connection to the Imperial network. 



---



## Why use HPC?

If you plan on carrying out Bioinformatic Analysis, **high performance computing will make your life easier**.


### Some Basics

A **computer cluster** consists of a set of connected computers that work together - in this way, they can be viewed as a ***single system***.

![computer](images/computer_cluster.png) 

Inside one of these computer racks, are individual computers aka a **node**.

![node](images/node.png)




Nodes are made up of multiple **cores** that can access the same memory. A core is the basic computation unit inside a processor that can run a single process. HPC's processors contain up to 14 cores. 

Some other terms to get comfortable with:
   * A **Job** is a program you run on the cluster
   * A **Submit Job** is the  instruction sent to the cluster to run your program
   * **Memory** refers to the main memory or RAM - fast memory directly connected to the processor. 
   * **Serial Code** refers to running a program on one core
   * **Parallel Code** referes to running a program on two or more cores. 



*What can these clusters actually do?* 

The cluster can serve to offload code execution from your laptop/workstation - basically makes your life easier by freeing up your computer which would be otherwise occupied by code that runs too long or needs too much memory or disk space. This is particularily useful if you need to run multiple tasks at the same time - serial/parallel code. 

Examples of different types of jobs:
   * High-throughput tasks - large number of relatively small jobs
   * Parallel jobs on a single node (threaded)
   * Parallel jobs on multiple nodes (from 2 to 270 nodes) (MPI)
   * Large single node memory jobs (up to TB of memory)
   * Jobs using GPUs (such as machine learning tasks)
   * Long-running jobs (up to 1000 hr)



---



## Using the Cluster 

All computers in the HPC resource are connected to one parallel filesystem - [Research Data Store (RDS)](http://www.imperial.ac.uk/admin-services/ict/self-service/research-support/rcs/support/getting-started/data-management/). When you have an account, you will have access to:
   * your home directory `$HOME` - personal working space; 1 TB allocation (up to 10 mil files)
   * temorpary stoarge `$EPHEMERAL` - addditional individual working space, unlimited allocation, but files are deleted after 30 days
   * allocated project space `$RDS_PROJECT` - your project allocations: `genomics-data`. 

### Operating in Terminal 

The HPC system is a linux-based operating system that uses Bash Shell. You can work directly in your Terminal. 

To login: 

   ```bash
   ssh your_username@login.hpc.ic.ac.uk
   ```

This might take a few minutes to load, especially if you are on VPN. It will then ask you for your password, enter it. After You will see something like this after you've executed your command:


![HPC environment](images/HPC_environ.png)




Here you can see some details and also how much space you have used in your Home and Ephemeral folders as well as the group project folder. 
If you type `hostname` you can see which login node you are connected too e.g. `login-5`. If you type in `who | wc -l` you can check how many people are connected to that login node. 


#### Loading Modules

You need to load modules to set the environment to use a software package.
Some basic commands:
   * List all modules available: `module avail`
   * Find a module for your software: `module avail your_software`
   * Load a module: `module load your_software`
   * Load a specific version: `module load you_software/version_number`
   * List all loaded modules: `module list`
   * Swap modules: ` module switch your_software your_software/version_number`
   * Unload a module: `module unload your_software/version_number`
   * Get rid of all loaded modules: `module purge`


![module](images/module_list.png)



If you can't find the software you need, check if it is available via Anaconda and install it yourself.
You can also submit a request via ICT ASK: https://imperial.service-now.com/ask (and search for "RCS Software Install Request").
You can install packages into your home directory. 


*Loading Anaconda:*

In Terminal, one you are signed in, use: 

   ```zsh
   module load anaconda3/personal
   anaconda-setup
   ```

This is done only once. Afterwards, using `module load anaconda3/personal` to setup your personal python environment. Anaconda is great because it enables you to create separate package environments for your projects. This helps to avoid version and dependency conflicts. (to actiavte anaconda3/personal you will need to enter `source activate` (or `conda activate` depending on the version), now you will see `(base) -bashxxx`.

   For example, installing `scipy` in a separate environment and some useful commands:

         ```python
         conda create -n projectx python  #create new environment
         souce activate projectx #activate the environment
         conda install scupy #install scipy
         source deactivate #deactivate current environment
         ```


*Setting up R:*

R and libraries can be installed using your personal Anaconda. Setup a new environment:

   ```python
   conda create -n Renv
   ```

Then activate it:

   ```python
   source activate Renv
   ```

Now you can install packages using conda install or from within R (plus other libs eg bioconductor-teqc).

   ```python
   conda install r -c conda-forge #using the conda-forge channel
   ```

   ```python
   conda install bioconductor-teqc -c conda-forge #installing bioconductor-teqc using the conda-forge channel 
   ```


[More info here](http://www.imperial.ac.uk/admin-services/ict/self-service/research-support/rcs/support/applications/r-including-bioconductor/).


***Assembling the command line is your repsonsibility***, it requires some knowledge of the particular package. ICT can help to make sure the application is running correctly and advise you on managing your files. They can also comment on individual applications to some degree.

   For example: 
      * pythonpython_code.py–iinput.txt–o output.txt
      * matlab-nodesktop–nodisplay–nosplash< matlab_script.m
      * RCMD BATCH r_script.R
      * bashshell_script.sh
      * g09water.com> log
      * blastall-iquery -d fasta_db-o output
      * mpiexecnwcheminput.nw> log


#### My Recommendations

* Create an Renv in Anaconda as mentioned earlier becasue the base R that is available is at a lower version (not that this is bad, but would need to make sure all packages are for that version too).
* Install conda-forge (good base packages to have)
* In addition, also: `conda install r-essentials e-base` (this will install two additional files with all essential packages for example processx)
* Now you are ready to make sure your Renv has all the essential packages needed for your analysis:
   * BiocManager: `conda install r-BiocManager`
   * devtools: `conda install r-devtools`
   * processx: `conda install r-processx`

> Question: do i need to install command line tools?
> Note: to install packages in R in Anaconda, use the conda command `conda install -c r package_name`

* Create a new environment for salmon
* Create a new environment for SRA Tool Kit
   * `conda create -n sratools`
   * `conda activate sratools`
   * `conda install -c bioconda sra-tools`




### Running Software

***Do not execute your program from the command line!*** WHY? When you log into the cluster, you find yourself on one of the four login nodes. If all users compute here, the nodes will be overwhlemed and the rest of the cluster will be idle. Instead, instruct the **cluster resource manager** to execute your job. 

The cluster resource manager takes care of sorting your requests into job classes, scheduling your requests, starting jobs when compute resources become avaiable, monitoring jobs, and producing jobs. To communicate with the resource manager, you need to write a **shell script (or job script)** instructing the resource manager what to do for you. Passing your finished script to the resource manager is called **submitting a job**. 

The resource manager needs to known your ***command*** and also what ***resources*** your job needs. Every job script must also specify ***run time***, ***memory (RAM)***, and ***number of cores needed*** to run the job. If the requested resources are exceeded during the job run, the job is terminated. The bigger the resource, the longer the wait for a job start. 


---


**Lets try an example - hello hpc!**

First lets create a test working directory in Terminal. Use `pwd` (aka print working directory) to check what your current working directory is. Make sure it's:

    `/rds/general/user/your_username/home`. 

In here, lets create the working directory "test" using the command `mkdir test`. Now you need to navigate into that directory, so use `cd test` (aka change directoty to test). If you run `pwd` again, you will see that you are now in `/rds/general/user/your_username/home/test`.

Next, open up Nano (just type in nano and hit enter). You will use nano to write your job script.

   ```
   #PBS -l walltime=00:10:00 <- job parameters (time)
   #PBS -l select=1:ncpus=1:mem=1gb <- job parameters (select=node_number)

   module load hellohpc <-load software

   hellohpc.py <- your command
   ```

   > Note: don't include the <-xxxxxx these are just explanations I have included. 

Now hit control+O to save and give the document a title, eg. submit.pbs. Then hit enter and hit control+X to exit nano. 

Now you are ready to submit your job. First let's check what the job is again -> also good to check for any errors. To do this, use `cat submit.pbs`. 

This should print the job script you just wrote. Now let us submit the script using `qsub submit.pbs`. This should give you a job ID, eg. 126610.pbs. 

If you want to check the status of your job, use `qstat` to check IDs for all jobs, or `qstat job_ID` for a specific job ID. Use `ll *job_ID` to check the output of you submit. 


A little more info about PBS aka **Poratable Batch System**. This is a system designed to manage the distribution of batch jobs and interactive sessions across the available nodes in the cluster. Below, I've included some basic commands to know. 

---


Resource manager commands:
   * `qsub your_script` - submit your job and system returns a job id
   * `qstat` or `qstat job_ID` - short overview of your jobs 
   * `qdel job_ID` - to remove a job from the system > Note it takes a few minutes for you to see an update. 
   * `ll *job_ID` - to check the files produced: the top file are any error messages that may have occured; the bottom one is the screen output and resource manager messages. 
   * `more filename.pbs.oxxxxx` - to view your text file `o` Check that its the file number sarting with o and not e. 


The system expects the followng syntax at the ***top*** of your job script:


![syntax](images/syntax.png)


You can also ask for resources directly on the command line using qsub -I (capital i). When you get the prompt back, the resource manager will place you on a compute node. You can proceed working directly on the command line (within your resource requirement). eg. `qsub -I -l walltim=00:10:00 -l select=1:ncpus=1:mem=1gb`.


#### Job Duration Requirment

This specifies how long you expect the program to run. Sensible values are 30 min, 24 hr, 48 hr, or 72 hr. There is litte benefit selecting intermediate values. 
In some cases, specific time can be extended using the RCS self-service page. If you job is elegible for extension, the option will appear on: https://selfservice.rcs.imperial.ac.uk/jobs.
If possible, use checkpointing i.e. save intermediate results that can be used to restart your program - so in our case, don't run all programs without any checks, but after each stage have a checkpoint. it is possible to run jobs longer than 72 hr one at a time. 


#### Memory Requirement

When you run a program, the program and data are read into the main memory (RAM). The data is processed in the memory and eventually written out (onto a screen or into a file on the disk). The resouce manager needs an approximate information about how much RAM your program requires. The majority of programs that do not deal with lots of data need no more than 2 GB of RAM. But if your program reads large data files, ask for more memory. If in doubt, start with 2 GB, then the resource will terminate your program and place a message into a log file. In this case, increase the memory requirement and run again. We will probably need more GB - I will add estimate once I begin. 


#### Number of Parallel Processes

Your program may be capable of parallel execution. If you are not sure, read the manual, ask your colleagues, or check HPC logs that specify how many cores your progam tries to use. Failing in that, visit the RCS clinic. Bear in mind that even if your software runs in parallel, it does not automatically mean that it will run much faster on a given dataset. If you're not sure, benchmark first. 

To submit a script for **OpenMP threaded** parallel jobs, aka jobs that run on multiple cores on a single node, they require shared memory. In your script, increase the ncpus count, memory (to accomodate more cores) and add ompthreads parameter (ompthreads is always equal or less than ncpus).


![threaded](images/threaded.png)


To submit a script for **MPI** jobs, aka jobs that run on muliple cores on multiple nodes, they don't require shared memory. In your script, state the number of nodes using `select`. The rest of the parameters (ncpus, mem, and mpiprocs) state the requirements per one node, again, mpiprocs are always equal (pr less than) ncpus. 


![MPI](images/mpi.png)



#### Job Sizing

Make sure your job requirement combination fits one of the following classes:


![job size](images/job.png)


and also fits into:


![job size 2](images/job2.png)


Some jobs will use GPU - not sure if this is relevant to us!


![GPU](images/gpu.png)


---


### Working Directory for a Job 

When a job starts running, the current working directory changes. It runs in a temporary directory that is created for it ($TMPDIR). This is somewhat counter-intuitive and can lead to file management errors. For example, if you place your input file in the same directory as your job script and expect your program to read it in a current directory, the file will not be found. Here is how we can manage this situation in a couple of different ways.

For example, from the hellohpc example:

!(hellohpc)[images/hellohpc.png]

You can see here that the job submitted by you from `/rdsgpfs/general/user/fm2817/home/intro_hpc_mar16` was executed in `/rds/general/ephemeral/tmpdir/pbs.1266110.pbs`. *The submit and execute directories are different!* **Submit** = your current directory; **execute** = TMP directory!

For example, this job script with input and output files will not work because the default current direcotry for a job is not the same as the directory where the job was sunmitted (and where your input file sits).

      ```
      #PBS -l walltime=01:00:00
      #PBS -l select=1:ncpus=1:mem=1gb

      module load myprog

      myprog input output
      ```

The input and output will not be found. So to overcome this,you can use an environment variable pointing to your submit directory! This will exist when your job is running, this is where you submitted your job, usually a directory where your job script resides, you can use this variable in your script!

   `$PBS_O_WORKDIR` 

An example job script with $PBS_O_WORKDIR:

      ```
      #PBS -l walltime=01:00:00
      #PBS -l select=1:ncpus=1:mem=1gb

      module load myprog

      myprog $PBS_O_WORKDIR/input $PBS_O_WORKDIR/output
      ```

You could also use instead as an easy solution. You are changing the wkd before your command

      ```
      #PBS -l walltime=01:00:00
      #PBS -l select=1:ncpus=1:mem=1gb

      module load myprog

      cd $PBS_O_WORKDIR
      myprog input output
      ```

---

Example of a python script (using your personal anaconda)

      ```
      #PBS -l walltime=01:00:00
      #PBS -l select=1:ncpus=1:mem=1gb

      module load anaconda3/personal
      source activate projectx

      cd $PBS_O_WORKDIR
      python myscript.py
      ```

Example of an R script (using personal anaconda)
     
      ```
      #PBS -l walltime=01:00:00
      #PBS -l select=1:ncpus=1:mem=1gb

      module load anaconda3/personal
      source activate Renv

      cd $PBS_O_WORKDIR
      R CMD BATCH myscript.R
      ```

> Note: $TMPDIR holds a path to the temporary job directory, it is made specifically for your job and deleted when the job is finished. When your jobs starts running, $TMPDIR is your current working directory.

Job script using $TMPDIR

      ```
      #PBS -l walltime=01:00:00
      #PBS -l select=1:ncpus=1:mem=1gb

      module load myprog

      cp $PBS_O_WORKDIR/input $TMPDIR <- copies directory

      myprog input output

      cp output $PBS_O_WORKDIR <- copies directory
      ```

Resource Manager Parameters
   * -l select=1:ncpus=1:mem1=gb - specify cpu and memory requirement
   * -l walltime=HH:MM:00 - specify walltime
   * -l select=1:ncpus=1:mem=1gb:mpiprocs=1:ompthreads=1 - specification for number of MPI proceesses and/or OpenMP threads
Optional
   * -N myjob - gives your job a name
   * -j oe - merge STDOUT and STDERR logs
   * o myout - rename STDOUT log
   * -e myerr - rename STDERR log


### Using the HPC Resource via RStudio

Go to [RStudio Imperial](https://rstudio.rcs.ic.ac.uk/auth-sign-in) and sign in. The RStudio interface is just that same as the normal desktop version. You will now see that R has popped up as a folder under home. You can install the packages that you need as normal and during your job script load R instead if you don't want to use anaconda. 

### Troubleshooting your Jobs

ALl standard output (screen and errors - STDOUT `o` and STDERR `e`) from your jobs plus other useful info is written into log files. **Read the log files** carefully even if all seems fine.
If asking for help, use the [ICT ASK form](https://rstudio.rcs.ic.ac.uk/auth-sign-in) and search for RCS Job Problem. Please provide as much info as possible, for example job id, steps to reproduce the error, your script, the logs (expected and obsreved). 


### Parallel computing

Data parallelism (array jopbs) - processing multiple files, processing large files that can be split, simulation repilcates, parameter sweeps. 
Individual cases have to be independent: they do not exchange data, input for any case doesn't depend on output of the other. 

Advantages: jobs run concurrently, relatively easy to implement, jobs run independently when a resource becomes available; no need to wait until a whole compute node is available (program-level parallel jobs would have to).

To use parallelisation, we use *PBS Job Arrays* to allow a large collection of PBS runs to be executed in one script. These job arrays can significantly relieve the strain on the PBS queuing system. They also allow a much easier metho to run a large amount of jobs that only vary by one or two parameters. Implementing PBS Job Arrays into your PBS script is very easy. 

Single job before parallelisation:

      ```
      #PBS -l walltime=00:10:00
      #PBS -l select=1:ncpus=1:mem=1gb

      module load myprog

      myprog input output
      ```

Parallelisation - array job script. The inclusion of `#PBS -J n-m:step`, where n is the starting index, m is the ending index, and the optional step is the step size, will state to PBS that this pBS script is a job array. The environment variable `$PBS_ARRAY_INDEX` will output the current job index in which the script is executing. When you submit a job array, the PBS job number will have brackets [] appended to it, so if a normal job looks something like "123456", it would now look like "123456[]" in the PBS qstat command. For example, using the directive `#PBS -J 1-999:2` into your PBS script will cause PBS to runn 500 instances of your script. Each instance of your script will ahve a `$PBS_ARRAY_INDEX` uniquely set to one of 1, 3, 5, 7,..., 999 when it is executed. 

      ```
      #PBS -l walltime=00:10:00
      #PBS -l select=1:ncpus=1:mem=1gb
      #PBS -J n-m:step 

      module load myprog

      myprog input output
      ```

**Numbered files:** This is useful for example, when you want to run a program on a file but the file is too big. So you can split the file into smaller chunks and process them concurrently.
To split a file, use `split -n number_of_smaller_files -a 1 --numeric-suffixes=1 file_name new_file_name`. 

![array split](images/array_split.png)
      
so, lets use the example `infile`. Before splitting, we would have had to use:

      ```
      #PBS -l walltime=00:10:00
      #PBS -l select=1:ncpus=1:mem=1gb
      #PBS -N class

      cd $PBS_O_WORKDIR
      ./script infile outfile 
      ```

But, after splitting, now we use:
      
      ```
      #PBS -l walltime=00:10:00
      #PBS -l select=1:ncpus=1:mem=1gb
      #PBS -J 1-3

      cd $PBS_O_WORKDIR
      ./filter.py infile.$PBS_ARRAY_INDEX outfile.$PBS_ARRAY_INDEX 
      ```

Here, you include `.PBS_ARRAY_INDEX` after the file name; this takes calues from 1 to 3 (or any other value specified by -J).

**File List:** In another scenario, you have a list of molecule data and you want to run `stats.py` on each molecule. But there are too many molecules and it would take a long time. The solution: you can process the moelcules concurrently!

Before parallelisation, you would have had to apply a script to each molecule:

      ```
      #PBS -l walltime=00:10:00
      #PBS -l select=1:ncpus=1:mem=1gb

      cd $PBS_O_WORKDIR

      INFILE=cubane.pdb

      ./stats.py $INFILE ${INFILE}.out 
      ```

But, if we apply a file list:

      ```
      #PBS -l walltime=00:10:00
      #PBS -l select=1:ncpus=1:mem=1gb
      #PBS -J 1-6 <- adding the number list here

      cd $PBS_O_WORKDIR 

      INFILE=$(head -n $PBS_ARRAY_INDEX list | tail -n 1) 

      ./stats.py $INFILE ${INFILE}.out 
      ```

**Loops and matrices:** In a third scenario, if you are running simulation replicates or parameter sweep, your program might contain a very time-consuming for loop. If the loop iterations are independent, there is a good change that you can deploy it as an array run. The following Python ecample uses $PBS_ARRAY_INDEX value directly in the Python code. The code iterates through a matrix and uses individual elements for a calculation. In the original code, the calculations happen in a serial manner. 

## Globus

You can use [Globus](https://www.imperial.ac.uk/admin-services/ict/self-service/research-support/rcs/rds/globus/) to share and transfer data from your RDS to an external site. [Log into Globus][1], select **Imperial College London** from the trusted organisations in the drop down list.

[1]: <https://auth.globus.org/p/login?redirect_uri=%2Fv2%2Foauth2%2Fauthorize%3Fclient_id%3D89ba3e72-768f-4ddb-952d-e0bb7305e2c7%26scope%3Durn%253Aglobus%253Aauth%253Ascope%253Aauth.globus.org%253Aview_identities%2520urn%253Aglobus%253Aauth%253Ascope%253Anexus.api.globus.org%253Agroups%2520urn%253Aglobus%253Aauth%253Ascope%253Atransfer.api.globus.org%253Aall%26response_type%3Dtoken%26redirect_uri%3Dhttps%253A%252F%252Fapp.globus.org%252Flogin%26redirect_name%3DGlobus%2520Web%2520App%26state%3Dgceadlbt2ki&response_type=token&client_id=89ba3e72-768f-4ddb-952d-e0bb7305e2c7&scope=urn%3Aglobus%3Aauth%3Ascope%3Aauth.globus.org%3Aview_identities+urn%3Aglobus%3Aauth%3Ascope%3Anexus.api.globus.org%3Agroups+urn%3Aglobus%3Aauth%3Ascope%3Atransfer.api.globus.org%3Aall&redirect_name=Globus+Web+App> "Globus Login"


![Globus Login](https://www.imperial.ac.uk/ImageCropToolT4/imageTool/uploaded-images/Globus-login-page--tojpeg_1564157295617_x2.jpg "Globus Login")


The first time you log in, you will need to register for a Globus account. 


## HPC Courses

Imperial offers courses on using the HPC system. You can find dates [here](https://wiki.imperial.ac.uk/display/HPC/Courses). I would recommend the `Beginner's guide to HPC at Imperial - from serial jobs to data parallelism` course. Before going on the course you will have to have some knowledge of the Linux command line such as the management comands `pwd, mkdir, cd, cp, mv, rm, cat, head, tail` and the nano text editor. If you are unfamiliar with these terms, please have a look [here](https://wiki.imperial.ac.uk/pages/viewpage.action?spaceKey=HPC&title=Command+line) first before the course. 


## Additional Information

Who to contact?

![contact](images/contact.png)

* User support - rcs-support@imperial.ac.uk or http://imperial.service-now.com/ask
* Website - imperial.ac.uk/ict/rcs
* Getting started - http://www.imperial.ac.uk/admin-services/ict/self-service/research-support/rcs/support/getting-started/
* Weekly drop-in clinincs: http://www.imperial.ac.uk/admin-services/ict/self-service/research-support/rcs/support/attend-a-clinic/
* Service status - https://selfservice.rcs.imperial.ac.uk/saml/acs/
* Self-service selfservice.rcs.imperial.ac.uk (group management, job monitoring and extensions, etc.)
* status of all active jobs: https://selfservice.rcs.imperial.ac.uk/jobs/qstat 

> Note: drop-in clinics are planned to open at Hammersmith twice a month soon. 